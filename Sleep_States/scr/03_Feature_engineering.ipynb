{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from pandas.api.types import is_datetime64_ns_dtype\n",
    "import gc  # garbage collector module\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.offline\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import kaleido\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # ignore any warnings in the code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the common path\n",
    "# change this to your local path\n",
    "path = 'gs://sleep-nov2023/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a Parquet file\n",
    "# train_series_aggregated = pd.read_parquet(path + 'full_train_and_split/train_series_aggregated.parquet')\n",
    "# X_test = pd.read_parquet(path + 'full_train_and_split/X_test.parquet')\n",
    "X_train = pd.read_parquet(path + 'full_train_and_split/X_train.parquet')\n",
    "# y_test = pd.read_parquet(path + 'full_train_and_split/y_test.parquet')\n",
    "y_train = pd.read_parquet(path + 'full_train_and_split/y_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "      <th>step</th>\n",
       "      <th>series_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-14 15:30:00</td>\n",
       "      <td>3.084700</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-14 15:31:00</td>\n",
       "      <td>68.460503</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>12.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-14 15:32:00</td>\n",
       "      <td>-79.998703</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>24.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-14 15:33:00</td>\n",
       "      <td>-80.008202</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>36.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-14 15:34:00</td>\n",
       "      <td>-80.025299</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>48.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp     anglez    enmo  step     series_id\n",
       "0 2018-08-14 15:30:00   3.084700  0.0229   0.0  038441c925bb\n",
       "1 2018-08-14 15:31:00  68.460503  0.0395  12.0  038441c925bb\n",
       "2 2018-08-14 15:32:00 -79.998703  0.0691  24.0  038441c925bb\n",
       "3 2018-08-14 15:33:00 -80.008202  0.0140  36.0  038441c925bb\n",
       "4 2018-08-14 15:34:00 -80.025299  0.0141  48.0  038441c925bb"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>night</th>\n",
       "      <th>event</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>1</td>\n",
       "      <td>onset</td>\n",
       "      <td>4992.0</td>\n",
       "      <td>2018-08-14 22:26:00</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>1</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>10932.0</td>\n",
       "      <td>2018-08-15 06:41:00</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>2</td>\n",
       "      <td>onset</td>\n",
       "      <td>20244.0</td>\n",
       "      <td>2018-08-15 19:37:00</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>2</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>27492.0</td>\n",
       "      <td>2018-08-16 05:41:00</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>3</td>\n",
       "      <td>onset</td>\n",
       "      <td>39996.0</td>\n",
       "      <td>2018-08-16 23:03:00</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      series_id  night   event     step           timestamp    year  month  \\\n",
       "0  038441c925bb      1   onset   4992.0 2018-08-14 22:26:00  2018.0    8.0   \n",
       "1  038441c925bb      1  wakeup  10932.0 2018-08-15 06:41:00  2018.0    8.0   \n",
       "2  038441c925bb      2   onset  20244.0 2018-08-15 19:37:00  2018.0    8.0   \n",
       "3  038441c925bb      2  wakeup  27492.0 2018-08-16 05:41:00  2018.0    8.0   \n",
       "4  038441c925bb      3   onset  39996.0 2018-08-16 23:03:00  2018.0    8.0   \n",
       "\n",
       "    day  hour  \n",
       "0  14.0  22.0  \n",
       "1  15.0   6.0  \n",
       "2  15.0  19.0  \n",
       "3  16.0   5.0  \n",
       "4  16.0  23.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise data\n",
    "# use standard scaler for `enmo`\n",
    "# use deviation by 90 for `anglez` because it is a 90 degree angle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train['enmo'] = StandardScaler().fit_transform(X_train[['enmo']])\n",
    "X_train['anglez'] = X_train['anglez'] / 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the locomotor Inactivity During Sleep (LIDS) out of the ENMO signal\n",
    "\n",
    "LIDS refers to \"Locomotor Inactivity During Sleep\", a concept derived from the non-linear conversion of locomotor activity. This conversion helps reveal movement patterns that correspond directly to ultradian sleep cycles, providing a means to replicate laboratory sleep parameters dynamically.\n",
    "\n",
    "Based on two different publications <sup>1, 2</sup>, we calculated `lids` = 100/ (activity count + 1), where activity count is computed using a 10-minute moving sum over max(0, `enmo_clean` − 0.02). LIDS is then smoothed using a moving average over a 30-min window.\n",
    "\n",
    "<sup>1</sup>\n",
    "<a\n",
    "    href=https://doi.org/10.1016/j.cub.2017.11.063>\n",
    "    Dynamics and Ultradian Structure of Human Sleep in Real Life\n",
    "</a>\n",
    "\n",
    "<sup>2</sup>\n",
    "<a\n",
    "    href=https://doi.org/10.1038/s41598-020-79217-x>\n",
    "    Sleep classification from wrist-worn accelerometer data using random forests\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ENMO - 0.02 and take max with 0\n",
    "activity_count = np.maximum(0, X_train['enmo'] - 0.02)\n",
    "\n",
    "# compute 10-minute moving sum\n",
    "activity_count_10min_sum = activity_count.rolling(window=10).sum()\n",
    "\n",
    "# calculate LIDS\n",
    "lids = 100 / (activity_count_10min_sum + 1)\n",
    "\n",
    "# add lids to the original sleep dataframe\n",
    "X_train['lids'] = lids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "      <th>step</th>\n",
       "      <th>series_id</th>\n",
       "      <th>lids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-14 15:30:00</td>\n",
       "      <td>0.034274</td>\n",
       "      <td>-0.358965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-14 15:31:00</td>\n",
       "      <td>0.760672</td>\n",
       "      <td>-0.285397</td>\n",
       "      <td>12.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-14 15:32:00</td>\n",
       "      <td>-0.888874</td>\n",
       "      <td>-0.154216</td>\n",
       "      <td>24.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-14 15:33:00</td>\n",
       "      <td>-0.888980</td>\n",
       "      <td>-0.398408</td>\n",
       "      <td>36.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-14 15:34:00</td>\n",
       "      <td>-0.889170</td>\n",
       "      <td>-0.397965</td>\n",
       "      <td>48.0</td>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    anglez      enmo  step     series_id  lids\n",
       "0 2018-08-14 15:30:00  0.034274 -0.358965   0.0  038441c925bb   NaN\n",
       "1 2018-08-14 15:31:00  0.760672 -0.285397  12.0  038441c925bb   NaN\n",
       "2 2018-08-14 15:32:00 -0.888874 -0.154216  24.0  038441c925bb   NaN\n",
       "3 2018-08-14 15:33:00 -0.888980 -0.398408  36.0  038441c925bb   NaN\n",
       "4 2018-08-14 15:34:00 -0.889170 -0.397965  48.0  038441c925bb   NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7778985, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate additional values like entropy and peak of amplitude during sleep duration\n",
    "Based on one publication <sup>1</sup>, we incorporated additional signal metrics that can be calculated.\n",
    "\n",
    "| Feature                   | Description                                           |\n",
    "|---------------------------|-------------------------------------------------------|\n",
    "| RollingMean_{window_size}min (anglez) | Rolling mean for a specified time window in 'anglez' |\n",
    "| RollingStd_{window_size}min (anglez)  | Rolling standard deviation for a specified time window in 'anglez' |\n",
    "| RollingMin_{window_size}min (anglez)  | Rolling minimum for a specified time window in 'anglez' |\n",
    "| RollingMax_{window_size}min (anglez)  | Rolling maximum for a specified time window in 'anglez' |\n",
    "| RollingMedian_{window_size}min (anglez) | Rolling median for a specified time window in 'anglez' |\n",
    "| Entropy_{column_name}_{resolution} (anglez) | Entropy for 'anglez' at specified resolution |\n",
    "| RollingMean_{window_size}min (enmo) | Rolling mean for a specified time window in 'enmo' |\n",
    "| RollingStd_{window_size}min (enmo)  | Rolling standard deviation for a specified time window in 'enmo' |\n",
    "| RollingMin_{window_size}min (enmo)  | Rolling minimum for a specified time window in 'enmo' |\n",
    "| RollingMax_{window_size}min (enmo)  | Rolling maximum for a specified time window in 'enmo' |\n",
    "| RollingMedian_{window_size}min (enmo) | Rolling median for a specified time window in 'enmo' |\n",
    "| Entropy_{column_name}_{resolution} (enmo) | Entropy for 'enmo' at specified resolution |\n",
    "| RollingMean_{window_size}min (lids) | Rolling mean for a specified time window in 'lids' |\n",
    "| RollingStd_{window_size}min (lids)  | Rolling standard deviation for a specified time window in 'lids' |\n",
    "| RollingMin_{window_size}min (lids)  | Rolling minimum for a specified time window in 'lids' |\n",
    "| RollingMax_{window_size}min (lids)  | Rolling maximum for a specified time window in 'lids' |\n",
    "| RollingMedian_{window_size}min (lids) | Rolling median for a specified time window in 'lids' |\n",
    "| Entropy_{column_name}_{resolution} (lids) | Entropy for 'lids' at specified resolution |\n",
    "\n",
    "Window size: 1 to 30 minutes (5-minute steps).\n",
    "Resolution: 'high' (bin size = 200) or 'low' (bin size = 20).\n",
    "\n",
    "<sup>1</sup>\n",
    "<a\n",
    "    href=https://doi.org/10.1038/s41598-020-79217-x>\n",
    "    Sleep classification from wrist-worn accelerometer data using random forests\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the rolling mean, std, min, max, and median\n",
    "# for different time windows from 1 minute to 30 minutes\n",
    "\n",
    "def calculate_stat_features(signal):\n",
    "    \"\"\"\n",
    "    Calculate rolling statistics for different time windows.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: pd.Series, input time-series data\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame, containing rolling mean, std, min, max, median\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    result_df = pd.DataFrame(index=signal.index)\n",
    "\n",
    "    # Define the list of window sizes (1 minute to 30 minutes with 5-minute steps)\n",
    "    window_sizes = range(1, 31, 5)\n",
    "\n",
    "    # Calculate rolling statistics for each window size\n",
    "    # By using .shift(1), you are effectively moving the values one step forward in time, \n",
    "    # which helps in aligning the rolling statistics with the original time series data, and it \n",
    "    # avoids NaN values in the first row.\n",
    "    for window_size in window_sizes:\n",
    "        rolling_window = signal.rolling(window=window_size)\n",
    "        result_df[f'RollingMean_{window_size}min'] = rolling_window.mean().shift(1)\n",
    "        result_df[f'RollingStd_{window_size}min'] = rolling_window.std().shift(1)\n",
    "        result_df[f'RollingMin_{window_size}min'] = rolling_window.min().shift(1)\n",
    "        result_df[f'RollingMax_{window_size}min'] = rolling_window.max().shift(1)\n",
    "        result_df[f'RollingMedian_{window_size}min'] = rolling_window.median().shift(1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the entropy of a signal\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_entropy_features(signal, column_name, resolution='high', bin_size=None):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a signal and return it as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: pd.Series or float, input signal\n",
    "    - column_name: str, name of the column for creating feature names\n",
    "    - resolution: str, 'high' or 'low'\n",
    "    - bin_size: int, number of bins for 'low' resolution entropy (default is 20, ignored for 'high' resolution)\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series or pd.DataFrame, containing entropy values\n",
    "    \"\"\"\n",
    "\n",
    "    entropy_values = pd.Series()\n",
    "\n",
    "    if pd.Series(signal).isna().all():\n",
    "        # Handle case where all values are NaN\n",
    "        entropy_values[f'Entropy_{column_name}_{resolution}'] = np.nan\n",
    "    elif isinstance(signal, float) or pd.Series(signal).isna().any():\n",
    "        # Handle case where signal is a single value or contains NaN values\n",
    "        entropy_values[f'Entropy_{column_name}_{resolution}'] = 0.0\n",
    "    else:\n",
    "        # Convert the signal to a numpy array if it's a pandas Series\n",
    "        signal_array = np.array(signal)\n",
    "\n",
    "        # Check if the resolution is 'low' or 'high'\n",
    "        if resolution == 'high':\n",
    "            # Use binning for high-resolution entropy\n",
    "            probabilities = np.histogram(signal_array[~np.isnan(signal_array)], bins=200, density=True)[0]\n",
    "            entropy_values[f'Entropy_{column_name}_{resolution}'] = entropy(probabilities, base=2)\n",
    "\n",
    "        elif resolution == 'low':\n",
    "            # Use binning for low-resolution entropy\n",
    "            if bin_size is None:\n",
    "                bin_size = 20  # Default bin size for 'low' resolution\n",
    "\n",
    "            # Calculate the number of bins based on the specified bin size\n",
    "            bins = np.linspace(min(signal_array), max(signal_array), bin_size + 1)\n",
    "\n",
    "            # Use histogram to count occurrences in each bin\n",
    "            counts, _ = np.histogram(signal_array[~np.isnan(signal_array)], bins=bins)\n",
    "            probabilities = counts / len(signal_array[~np.isnan(signal_array)])\n",
    "            entropy_values[f'Entropy_{column_name}_{resolution}'] = entropy(probabilities, base=2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resolution. Use 'low' or 'high'.\")\n",
    "\n",
    "    return entropy_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to process\n",
    "columns_to_process = ['enmo', 'anglez', 'lids']\n",
    "\n",
    "# Results storage\n",
    "all_stats = []\n",
    "all_entropy_high = []\n",
    "all_entropy_low = []\n",
    "\n",
    "# Set the chunk size based on your available memory\n",
    "chunk_size = 1000000\n",
    "\n",
    "unique_series_ids = X_train['series_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each column\n",
    "for column in columns_to_process:\n",
    "    # Initialize lists to store results for each chunk\n",
    "    chunk_stats = []\n",
    "    chunk_entropy_high = []\n",
    "    chunk_entropy_low = []\n",
    "\n",
    "    # Iterate over chunks of the dataset\n",
    "    for start in range(0, len(X_train), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk_data = X_train.iloc[start:end]\n",
    "\n",
    "        # Calculate statistics for the current chunk\n",
    "        stats = chunk_data.groupby(['series_id', 'step'])[column].apply(calculate_stat_features)\n",
    "        stats.columns = [f'{column}_{col}' for col in stats.columns]\n",
    "        chunk_stats.append(stats)\n",
    "\n",
    "        # Calculate entropy for high resolution for the current chunk\n",
    "        entropy_high = chunk_data.groupby(['series_id', 'step'])[column].apply(\n",
    "            lambda x: calculate_entropy_features(x, column_name=column, resolution='high')\n",
    "        )\n",
    "        entropy_high.index = [f'{column}_{index}' for index in entropy_high.index]\n",
    "        chunk_entropy_high.append(entropy_high)\n",
    "\n",
    "        # Calculate entropy for low resolution for the current chunk\n",
    "        entropy_low = chunk_data.groupby(['series_id', 'step'])[column].apply(\n",
    "            lambda x: calculate_entropy_features(x, column_name=column, resolution='low', bin_size=20)\n",
    "        )\n",
    "        entropy_low.index = [f'{column}_{index}' for index in entropy_low.index]\n",
    "        chunk_entropy_low.append(entropy_low)\n",
    "\n",
    "    # Concatenate the results for each column and append to the final lists\n",
    "    all_stats.append(pd.concat(chunk_stats, keys=unique_series_ids))\n",
    "    all_entropy_high.append(pd.concat(chunk_entropy_high, keys=unique_series_ids))\n",
    "    all_entropy_low.append(pd.concat(chunk_entropy_low, keys=unique_series_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the results for each column and append to the final lists\n",
    "all_stats_df = pd.concat(all_stats, keys=unique_series_ids)\n",
    "all_entropy_high_df = pd.concat(all_entropy_high, keys=unique_series_ids)\n",
    "all_entropy_low_df = pd.concat(all_entropy_low, keys=unique_series_ids)\n",
    "\n",
    "# Concatenate the final results for all series_ids\n",
    "X_train = pd.concat([X_train, all_stats_df, all_entropy_high_df, all_entropy_low_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the modified X_train to the data folder\n",
    "X_train.to_parquet(path + 'full_train_and_split/X_train_modified.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Summary\n",
    "\n",
    "1. **Data Normalisation:**\n",
    "   - The 'enmo' column is normalised using the Standard Scaler.\n",
    "   - The 'anglez' column is scaled by dividing it by 90.\n",
    "\n",
    "2. **Calculation of Locomotor Inactivity During Sleep (LIDS):**\n",
    "   - LIDS is calculated based on the formula: `lids = 100 / (activity count + 1)`, where activity count is computed using a 10-minute moving sum over max(0, `enmo_clean` − 0.02).\n",
    "   - The activity count is derived as the maximum of 0 and the difference between 'enmo' and 0.02.\n",
    "   - LIDS is smoothed using a 30-minute moving average.\n",
    "\n",
    "3. **Rolling Statistics Calculation:**\n",
    "   - A function `calculate_stat_features` is defined to calculate rolling mean, standard deviation, minimum, maximum, and median for different time windows (1 minute to 30 minutes with 5-minute steps).\n",
    "   - The functions are applied to the 'enmo', 'anglez', and 'lids' columns, resulting in statistical features for each.\n",
    "\n",
    "4. **Entropy Calculation:**\n",
    "   - A function `calculate_entropy_features` is defined to calculate entropy of a signal.\n",
    "   - Entropy is calculated at both high and low resolutions (20 bins for low resolution).\n",
    "   - The calculated entropy features are added to the dataset for 'enmo', 'anglez', and 'lids' columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('series_id').describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('series_id').apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of NaN values within each series_id\n",
    "nan_percentage = X_train.groupby('series_id').apply(lambda x: (x.isnull().sum() / len(x)) * 100)\n",
    "\n",
    "# Display the result\n",
    "print(nan_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for missing data percentage\n",
    "threshold = 30\n",
    "\n",
    "# Calculate the percentage of NaN values within each series_id\n",
    "nan_percentage = X_train.groupby('series_id').apply(lambda x: (x.isnull().sum() / len(x)) * 100)\n",
    "\n",
    "# Filter features with >30% missing data within one series_id\n",
    "features_with_high_nan = nan_percentage[nan_percentage > threshold]\n",
    "\n",
    "# Display the result\n",
    "features_with_high_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
